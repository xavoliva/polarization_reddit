{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/polarization_reddit\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd '/workspaces/polarization_reddit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# from load.utils import load_comments\n",
    "from load.utils import (\n",
    "    load_users,\n",
    "    load_user_party,\n",
    "    load_user_party_parquet,\n",
    "    load_subreddits,\n",
    "    load_comments,\n",
    "    save_df_as_json,\n",
    "    save_df_as_parquet,\n",
    "    load_comments_dask,\n",
    ")\n",
    "from preprocessing.utils import (\n",
    "    tokenize_comment,\n",
    "    calculate_user_party,\n",
    "    load_event_comments,\n",
    "    save_event_comments,\n",
    "    build_vocab,\n",
    "    save_event_vocab,\n",
    ")\n",
    "\n",
    "from preprocessing.constants import (\n",
    "    EVENTS_DIR,\n",
    "    ELECTIONS_REGEX,\n",
    "    MIN_OCCURENCE_FOR_VOCAB,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR = 2008\n",
    "START_MONTH = 1\n",
    "STOP_MONTH = 12\n",
    "\n",
    "EVENT_NAME = f\"us_election_{YEAR}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Affiliation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = load_subreddits()[[\"subreddit\", \"party\"]]\n",
    "\n",
    "# TODO: take into account network structure to find other partisan subreddits\n",
    "# which are not labeled\n",
    "# Filter partisan subreddits\n",
    "subreddits = subreddits[subreddits[\"party\"].isin({\"dem\", \"rep\"})]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>party</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dem</th>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rep</th>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       subreddit\n",
       "party           \n",
       "dem           43\n",
       "rep           19"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddits.groupby(\"party\").count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load comments...\n",
      "Merge party information to comments...\n"
     ]
    }
   ],
   "source": [
    "print(\"Load comments...\")\n",
    "\n",
    "comments = load_comments_dask(\n",
    "    year=YEAR,\n",
    "    start_month=START_MONTH,\n",
    "    stop_month=START_MONTH,\n",
    ")[[\"author\", \"subreddit\"]]\n",
    "\n",
    "print(\"Merge party information to comments...\")\n",
    "comments_party = comments.merge(subreddits, on=\"subreddit\", how=\"inner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "print(len(comments_party))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_party = comments_party.groupby(by=\"author\").apply(\n",
    "    calculate_user_party,\n",
    "    meta={\n",
    "        \"dem_cnt\": \"int\",\n",
    "        \"rep_cnt\": \"int\",\n",
    "        \"score\": \"int\",\n",
    "        \"party\": \"string\",\n",
    "    },\n",
    ")\n",
    "user_party = user_party[user_party[\"score\"] != 0].compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_party = user_party.reset_index()\n",
    "user_party[\"author\"] = user_party[\"author\"].astype(\"string\")\n",
    "user_party[\"party\"] = user_party[\"party\"].astype(\"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr of users: 18\n",
      "dem    18\n",
      "Name: party, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nr of users: {len(user_party)}\")\n",
    "\n",
    "print(user_party.groupby(by=\"party\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df_as_parquet(\n",
    "    data=user_party,\n",
    "    target_file=f\"user_party_{YEAR}.parquet\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter event comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Load user party\")\n",
    "# user_party = load_user_party_parquet(year=YEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = load_comments_dask(\n",
    "    year=YEAR,\n",
    "    start_month=START_MONTH,\n",
    "    stop_month=STOP_MONTH,\n",
    ")\n",
    "\n",
    "user_comments = comments.merge(\n",
    "    user_party,\n",
    "    right_on=\"author\",\n",
    "    left_on=\"author\",\n",
    "    how=\"inner\",\n",
    ")\n",
    "\n",
    "# Filter event data based on keywords\n",
    "event_comments = user_comments[\n",
    "    user_comments[\"body_cleaned\"].str.contains(\n",
    "        ELECTIONS_REGEX[YEAR],\n",
    "        regex=True,\n",
    "    )\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and stem comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_comments[\"tokens\"] = event_comments[\"body_cleaned\"].apply(\n",
    "    tokenize_comment,\n",
    "    meta=(\"tokens\", \"string\"),\n",
    ").persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nr of event comments: 5964\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nr of event comments: {len(event_comments)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_event_comments(event_comments, EVENT_NAME, file_type=\"parquet\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build event vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read event data\n",
    "# events_comments= load_event_comments(event_comments, EVENT_NAME, file_type=\"parquet\")\n",
    "\n",
    "event_vocab = build_vocab(\n",
    "    event_comments[\"tokens\"],\n",
    "    min_words=MIN_OCCURENCE_FOR_VOCAB,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2443\n"
     ]
    }
   ],
   "source": [
    "print(len(event_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_event_vocab(event_vocab, EVENT_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
