{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/dss/dsshome1/03/di93fup/polarization_reddit\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%cd '..'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from word_partisanship.utils import (\n",
    "    logodds_with_prior,\n",
    ")\n",
    "from preprocessing.utils import (\n",
    "    split_by_party,\n",
    "    load_event_comments,\n",
    "    load_event_vocab,\n",
    "    build_vocab,\n",
    "    build_term_vector,\n",
    ")\n",
    "from preprocessing.constants import OUTPUT_DIR, MIN_OCCURENCE_FOR_VOCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"data/logs/word_partisanship.log\"),\n",
    "        logging.StreamHandler(stream=sys.stdout)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR = 2016\n",
    "\n",
    "EVENT_NAME = f\"us_elections_{YEAR}\"\n",
    "\n",
    "N_DISPLAY = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read event data\n",
    "event_comments = load_event_comments(event_name=EVENT_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_comments, rep_comments = split_by_party(\n",
    "    comments=event_comments,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-08 16:41:41,345 [INFO] (2782950, 9)\n",
      "2023-06-08 16:41:41,346 [INFO] (1526884, 9)\n"
     ]
    }
   ],
   "source": [
    "logging.info(dem_comments.shape)\n",
    "logging.info(rep_comments.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-08 16:41:41,436 [INFO] Building overall term vector...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mBuilding overall term vector...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m term_vec \u001b[39m=\u001b[39m build_term_vector(event_comments[\u001b[39m\"\u001b[39;49m\u001b[39mtokens\u001b[39;49m\u001b[39m\"\u001b[39;49m], event_vocab)\n\u001b[1;32m      3\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mBuilding dem term vector...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m dem_term_vec \u001b[39m=\u001b[39m build_term_vector(dem_comments[\u001b[39m\"\u001b[39m\u001b[39mtokens\u001b[39m\u001b[39m\"\u001b[39m], event_vocab)\n",
      "File \u001b[0;32m~/polarization_reddit/preprocessing/utils.py:174\u001b[0m, in \u001b[0;36mbuild_term_vector\u001b[0;34m(corpus, vocabulary)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild_term_vector\u001b[39m(corpus: pd\u001b[39m.\u001b[39mSeries, vocabulary) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m csr_matrix:\n\u001b[1;32m    169\u001b[0m     vec \u001b[39m=\u001b[39m CountVectorizer(\n\u001b[1;32m    170\u001b[0m         analyzer\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mword\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    171\u001b[0m         ngram_range\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m),\n\u001b[1;32m    172\u001b[0m         vocabulary\u001b[39m=\u001b[39mvocabulary,\n\u001b[1;32m    173\u001b[0m     )\n\u001b[0;32m--> 174\u001b[0m     doc_term_vec: csr_matrix \u001b[39m=\u001b[39m vec\u001b[39m.\u001b[39;49mtransform([corpus\u001b[39m.\u001b[39;49mstr\u001b[39m.\u001b[39;49mcat(sep\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m)])  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m    175\u001b[0m     \u001b[39mreturn\u001b[39;00m doc_term_vec\n",
      "File \u001b[0;32m~/.conda/envs/pol_env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1432\u001b[0m, in \u001b[0;36mCountVectorizer.transform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1429\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_vocabulary()\n\u001b[1;32m   1431\u001b[0m \u001b[39m# use the same matrix-building strategy as fit_transform\u001b[39;00m\n\u001b[0;32m-> 1432\u001b[0m _, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, fixed_vocab\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   1433\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[1;32m   1434\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/pol_env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1274\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1272\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[1;32m   1273\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[0;32m-> 1274\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[1;32m   1275\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1276\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/.conda/envs/pol_env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:111\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m     \u001b[39mif\u001b[39;00m preprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[1;32m    112\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m         doc \u001b[39m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[0;32m~/.conda/envs/pol_env/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:69\u001b[0m, in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[39mapply to a document.\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39m    preprocessed string\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39mif\u001b[39;00m lower:\n\u001b[0;32m---> 69\u001b[0m     doc \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39;49mlower()\n\u001b[1;32m     70\u001b[0m \u001b[39mif\u001b[39;00m accent_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m     doc \u001b[39m=\u001b[39m accent_function(doc)\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "event_vocab = build_vocab(event_comments[\"tokens\"], ngram_range=(1, 2), min_df=MIN_OCCURENCE_FOR_VOCAB)\n",
    "\n",
    "logging.info(\"Building overall term vector...\")\n",
    "term_vec = build_term_vector(event_comments[\"tokens\"], event_vocab)\n",
    "logging.info(\"Building dem term vector...\")\n",
    "dem_term_vec = build_term_vector(dem_comments[\"tokens\"], event_vocab)\n",
    "logging.info(\"Building rep term vector...\")\n",
    "rep_term_vec = build_term_vector(rep_comments[\"tokens\"], event_vocab)\n",
    "\n",
    "logging.info(\"Calculating loggodds...\")\n",
    "logodds = logodds_with_prior(\n",
    "    term_vec,\n",
    "    dem_term_vec,\n",
    "    rep_term_vec,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top token indices\n",
    "sorted_logodds_indices = np.argsort(logodds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort vocabulary tokens by index\n",
    "vocab_tokens = np.array(\n",
    "    [\n",
    "        key\n",
    "        for key, _ in sorted(\n",
    "            event_vocab.items(),\n",
    "            key=lambda item: item[1],\n",
    "        )\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Democrat tokens\")\n",
    "dem_idiosyncratic_tokens = list(vocab_tokens[sorted_logodds_indices[-N_DISPLAY:]])\n",
    "logging.info(dem_idiosyncratic_tokens)\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/{EVENT_NAME}_dem_idiosyncratic_tokens.json\", \"w\") as f:\n",
    "    json.dump(dem_idiosyncratic_tokens, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Republican tokens\")\n",
    "rep_idiosyncratic_tokens = list(vocab_tokens[sorted_logodds_indices[:N_DISPLAY]])\n",
    "logging.info(rep_idiosyncratic_tokens)\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/{EVENT_NAME}_rep_idiosyncratic_tokens.json\", \"w\") as f:\n",
    "    json.dump(rep_idiosyncratic_tokens, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pol_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0517ac6396832503edeb22d4ae2d55ad6af9f111efda9985705546d6640f6543"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
