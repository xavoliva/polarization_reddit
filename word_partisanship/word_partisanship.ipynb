{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "/home/xavi_oliva/Documents/Github/polarization_reddit\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%cd '..'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from word_partisanship.utils import (\n",
    "    logodds_with_prior,\n",
    ")\n",
    "from preprocessing.utils import (\n",
    "    split_by_party,\n",
    "    load_event_comments,\n",
    "    build_vocab,\n",
    "    build_term_vector,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR = 2016\n",
    "\n",
    "EVENT_NAME = f\"us_elections_{YEAR}\"\n",
    "\n",
    "N_DISPLAY = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read event data\n",
    "event_comments= load_event_comments(EVENT_NAME, file_type=\"parquet\")\n",
    "\n",
    "dem_comments, rep_comments = split_by_party(event_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384674\n",
      "75739\n"
     ]
    }
   ],
   "source": [
    "print(len(dem_comments))\n",
    "print(len(rep_comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54406389 46468953 7937421\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(event_comments[\"tokens\"], min_words=1)\n",
    "\n",
    "term_vec = build_term_vector(event_comments[\"tokens\"], vocab)\n",
    "dem_term_vec = build_term_vector(dem_comments[\"tokens\"], vocab)\n",
    "rep_term_vec = build_term_vector(rep_comments[\"tokens\"], vocab)\n",
    "\n",
    "logodds = logodds_with_prior(\n",
    "    term_vec,\n",
    "    dem_term_vec,\n",
    "    rep_term_vec,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_ind = np.argsort(logodds)\n",
    "sorted_vocab = np.array(sorted(vocab, key=vocab.get))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Democrat tokens\n",
      "['not respond' 'guidelin url' 'ref to' 'sand 2016' 'pleas read'\n",
      " 'be respond' 'mod at' 'hello' 'autom' 'post' 'her' 'url pleas'\n",
      " 'pleas mess' 'you can' 'in er' 'follow' 'submit url' 'thi remov'\n",
      " 'remov pleas' 'mess the' 'been remov' 'remov for' 'submit has'\n",
      " 'the subreddit' 'thi link' 'cont' 'url' 'titl' 'the mod' 'respond'\n",
      " 'rul in' 'mod' 'wil not' 'respond to' 'the commun' 'thi com' 'hil'\n",
      " 'commun' 'the follow' 'she' 'thi submit' 'clinton' 'thi' 'guidelin'\n",
      " 'commun guidelin' 'pleas' 'sand' 'remov' 'berny' 'submit']\n"
     ]
    }
   ],
   "source": [
    "print(\"Democrat tokens\")\n",
    "print(sorted_vocab[sorted_ind[-N_DISPLAY:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Republican tokens\n",
      "['trump' 'cruz' 'trump is' 'rand' 'he' 'immigr' 'muslim' 'donald'\n",
      " 'trump has' 'islam' 'abov link' 'conserv' 'illeg' 'jeb' 'pleas respect'\n",
      " 'url foot' 'subreddit contact' 'bloop someon' 'info subreddit'\n",
      " 'contact bot' 'bot bleep' 'threads info' 'bleep bloop' 'illeg immigr'\n",
      " 'bleep' 'foot if' 'anoth plac' 'bloop' 'thread from' 'rubio'\n",
      " 'donald trump' 'reddit subreddit' 'has link' 'follow any' 'fox' 'his'\n",
      " 'link pleas' 'the trump' 'lib' 'that trump' 'paul' 'you follow'\n",
      " 'from anoth' 'him' 'trump support' 'stump' 'respect the' 'ted'\n",
      " 'trump wil' 'someon has']\n"
     ]
    }
   ],
   "source": [
    "print(\"Republican tokens\")\n",
    "print(sorted_vocab[sorted_ind[:N_DISPLAY]])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15 (default, Sep 15 2021, 14:20:42) [GCC]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
